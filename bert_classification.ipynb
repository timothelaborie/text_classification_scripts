{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from typing import Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "model_name = 'microsoft/deberta-v3-large'\n",
    "output_dir = \"data/\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(output_dir + \"finance_sentiment.csv\")\n",
    "data_sample = data.sample(n=3000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timothe/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:435063810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timothe/anaconda3/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings\n",
    "print(\"model parameters:\" + str(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data_sample[\"label\"].tolist()\n",
    "labels = [0 if x == 0 else 1 for x in labels] # Labels: 0 -> Negative; 1 -> Positive\n",
    "# convert labels to one hot vectors\n",
    "labels = np.eye(2)[labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1214a13bc8e43f19727240b93211764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c505c8bdfcd4cbc923b2bf65c687740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data,val_data, train_labels, val_labels = train_test_split(data_sample[\"text\"], labels, test_size=1000/len(data_sample), random_state=42)\n",
    "dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(train_data, train_labels)])\n",
    "val_dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(val_data, val_labels)])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.659165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.253105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.222520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.6700\n",
      "Evaluation Accuracy: 0.9260\n",
      "Evaluation Accuracy: 0.9420\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl,training_args\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Assuming the evaluation dataset has 'labels' and 'predictions' fields\n",
    "        eval_dataloader = kwargs['eval_dataloader']\n",
    "        model = kwargs['model']\n",
    "        tokenizer = kwargs['tokenizer']\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in eval_dataloader:\n",
    "            inputs = batch['input_ids'].to(args.device)\n",
    "            labels = batch['labels'].to(args.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            labels = torch.argmax(labels, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=training_args.OptimizerNames.ADAMW_TORCH,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        num_train_epochs=1,\n",
    "        # report_to=\"wandb\",\n",
    "        report_to=\"none\",\n",
    "        group_by_length=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "    ),\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, output_dir + \"bert.pt\")\n",
    "torch.save(tokenizer, output_dir + \"bert_tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(output_dir + \"bert.pt\")\n",
    "# tokenizer = torch.load(output_dir + \"bert_tokenizer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: $OLLI - Ollie's Bargain Outlet EPS beats by $0.03, beats on revenue https://t.co/hgb3KCVBXc\n",
      "Score: 0.9726799726486206\n",
      "\n",
      "Text: Twitter Beats Revenue, User Growth Estimates in Fourth Quarter\n",
      "Score: 0.971839427947998\n",
      "\n",
      "Text: Highlight: “There’s going to be a bloodbath in terms of synergy savings and retrenchment…” @InvescoUS's… https://t.co/YPgyDW3rMX\n",
      "Score: 0.06798333674669266\n",
      "\n",
      "Text: $XLF $FAS $FAZ - Banks may face legal actions over margin calls - FT https://t.co/SAACWM7yNa\n",
      "Score: 0.06219761073589325\n",
      "\n",
      "Text: $USA $CRF $SCHX - It's time to buy stocks - Morgan Stanley's Wilson https://t.co/sk5Ll4yTei\n",
      "Score: 0.9677207469940186\n",
      "\n",
      "Text: $WTRH back over $1\n",
      "Score: 0.9764747023582458\n",
      "\n",
      "Text: Oil boosted by renewed hopes for global production cut https://t.co/4tAO1U31nz\n",
      "Score: 0.9700902700424194\n",
      "\n",
      "Text: $OIBR.C - Oi S.A. Is Transforming Into A Leading Telecom Infrastructure Wholesaler For 5G In Brazil. Sign up for up… https://t.co/XQk239OsBs\n",
      "Score: 0.9704325199127197\n",
      "\n",
      "Text: China to Suspend Additional Tariffs on Certain U.S. Goods\n",
      "Score: 0.8744943737983704\n",
      "\n",
      "Text: History shows that the Dow, S&P 500, and the Nasdaq could be in for another huge run in 2020.… https://t.co/xYovuKPQss\n",
      "Score: 0.9703984260559082\n",
      "\n",
      "Text: ICYMI: Oil spiked after the U.S. airstrike killed a top Iranian commander in Baghdad.  The strike escalates an alre… https://t.co/biC7W3GoKK\n",
      "Score: 0.9655750393867493\n",
      "\n",
      "Text: TUNE IN: Tesla's bull run is just getting started, says Ark's Cathie Wood. Watch #WallStreetWeek live today at 6pm... https://t.co/DvWmT4j4f5\n",
      "Score: 0.9692564010620117\n",
      "\n",
      "Text: Ford shakes up top management 3 days after weak profit outlook\n",
      "Score: 0.054509345442056656\n",
      "\n",
      "Text: $COMDX: Natural gas inventory showed a draw of 201 bcf vs a 92 bcf draw last week https://t.co/CGfYWf1Unq\n",
      "Score: 0.960127592086792\n",
      "\n",
      "Text: New drugs contribute to AbbVie's upbeat 2020 view; shares hit year-high\n",
      "Score: 0.9749807715415955\n",
      "\n",
      "Text: Stock Market Update: TJX remains bright spot in retail space today\n",
      "Score: 0.9775779247283936\n",
      "\n",
      "Text: Kenya GDP growth to drop to 3% or less in 2020 due to novel coronavirus - finance minister #economy #MarketScreener… https://t.co/o7Zmwn9GT6\n",
      "Score: 0.06516959518194199\n",
      "\n",
      "Text: NTPC Q3 Results: Profit Meets Estimates As Other Income Rises\n",
      "Score: 0.9717010855674744\n",
      "\n",
      "Text: Home Depot Falls Sharply After Release of 3rd-Quarter Results\n",
      "Score: 0.061851851642131805\n",
      "\n",
      "Text: Umicore reports 2019 profit beat on converters, recycling\n",
      "Score: 0.9709662795066833\n",
      "\n",
      "Text: Gold's haven reputation took a serious beating, with prices tumbling as investors sought to free up cash amid a bro… https://t.co/59kM1nufrI\n",
      "Score: 0.061898380517959595\n",
      "\n",
      "Text: AT&T stock falls after MoffettNathanson downgrades, saying dividend looks less compelling\n",
      "Score: 0.05347219109535217\n",
      "\n",
      "Text: Dassault Systemes Profit Outlook Lags After Virus Warning\n",
      "Score: 0.05662643536925316\n",
      "\n",
      "Text: Bear Signal Calls BIG Stock’s Rally Reversal\n",
      "Score: 0.0620230995118618\n",
      "\n",
      "Text: Fmr. White House advisor says Trump will slap tariffs on China in December\n",
      "Score: 0.945584237575531\n",
      "\n",
      "Text: Futures back off after four-day rally ahead of crucial jobs report\n",
      "Score: 0.06723403185606003\n",
      "\n",
      "Text: Baker Hughes started at buy with $32 stock price target at Deutsche Bank\n",
      "Score: 0.9720557928085327\n",
      "\n",
      "Text: In  1.60 $NYMT on news 6000 shares. Stock fell 80% and now can run big\n",
      "Score: 0.9637401103973389\n",
      "\n",
      "Text: Stock Market Update: ConocoPhillips forecasts $50 billion free cash flow over ten years\n",
      "Score: 0.975737988948822\n",
      "\n",
      "Text: Iranian Strike on U.S. Forces Roils Markets #index #MarketScreener https://t.co/fU5DTtl5Q5 https://t.co/JQHmnd54oH\n",
      "Score: 0.8697463870048523\n",
      "\n",
      "Text: Conn's did not CONNect on earnings today as comps were weak https://t.co/z6yrORwrby $CONN\n",
      "Score: 0.06363817304372787\n",
      "\n",
      "Text: BofA Upgrades WestRock After Containerboard Conference, London Pulp Week\n",
      "Score: 0.9768723249435425\n",
      "\n",
      "Text: $AMBA - Ambarella Will Be Hard Pressed To Keep Going At Its Current Pace. Read more: https://t.co/ihnEpI8rfA… https://t.co/JHXvj8hVEm\n",
      "Score: 0.07099755108356476\n",
      "\n",
      "Text: $TCO: Taubman Centers beats by $0.07, beats on revs -- co to be acquired by Simon Property (SPG) for $52.50/share i… https://t.co/JMpBxoahSh\n",
      "Score: 0.9740914702415466\n",
      "\n",
      "Text: A man is suing Burger King because the meatless Impossible Whopper is cooked on the same grill as meat products, th… https://t.co/2WEnjgoIr1\n",
      "Score: 0.06602519005537033\n",
      "\n",
      "Text: S&P may cut Qatar Insurance ratings over a $310 million loan https://t.co/WPluJ9zKYB https://t.co/S2xEY8RQRW\n",
      "Score: 0.06811797618865967\n",
      "\n",
      "Text: U.S. markets jump at the open https://t.co/nb9TiXrMa0 https://t.co/hiuEonQvUq\n",
      "Score: 0.9622417688369751\n",
      "\n",
      "Text: Highlight: “About 21 million investors in the U.S. would have an interest in investing in bitcoin,”… https://t.co/wJRda3ylEH\n",
      "Score: 0.9688079953193665\n",
      "\n",
      "Text: $NGM $NGM $ICPT - Big partner boosts biotech with 120% upside - Raymond James https://t.co/sqrqPvLRC9\n",
      "Score: 0.97343510389328\n",
      "\n",
      "Text: $USO $OIL $UCO - Crude inventories build sharply https://t.co/Ze5rjTzPYv\n",
      "Score: 0.9639325737953186\n",
      "\n",
      "Text: Volkswagen reveals exotic-sounding ID. Space Vizzion electric wagon at Los Angeles Auto Show\n",
      "Score: 0.9720884561538696\n",
      "\n",
      "Text: Newmont Goldcorp Is Poised to Outperform\n",
      "Score: 0.9716089367866516\n",
      "\n",
      "Text: $BIIB BOOM! December $310C at $11! https://t.co/TjNJQJiMfK\n",
      "Score: 0.9667574763298035\n",
      "\n",
      "Text: Uber may be about to lose the right to operate in London https://t.co/PWXX1xSAOj\n",
      "Score: 0.06152322143316269\n",
      "\n",
      "Text: Highlight: Cruise stocks are rallying. \"The industry has done a total 180 in the last few months,\" @cfraresearch's… https://t.co/oTvX5UGlnh\n",
      "Score: 0.9638152122497559\n",
      "\n",
      "Text: 50 $SPY PUTS DAYTRADE 4/6 $258P .36 AVG\n",
      "Score: 0.9787490963935852\n",
      "\n",
      "Text: $ARTL - Artelo Biosciences EPS misses by $0.13 https://t.co/tfIv9fui86\n",
      "Score: 0.06502483040094376\n",
      "\n",
      "Text: Silver Price Forecast – Silver Markets Continue Forming Support\n",
      "Score: 0.968752920627594\n",
      "\n",
      "Text: Global stock rally spurs selloff of Treasurys ahead of benchmark debt auction https://t.co/DqNyYNisgU\n",
      "Score: 0.06985142827033997\n",
      "\n",
      "Text: Estee Lauder shares jump premarket after earnings beat\n",
      "Score: 0.9710385799407959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch = val_data[:50].tolist()\n",
    "    tokens = tokenizer(batch, padding=True, return_tensors=\"pt\")\n",
    "    tokens = {k: v.cuda() for k, v in tokens.items()}\n",
    "    output = model(**tokens)\n",
    "    logits = output[0].cpu()\n",
    "    scores = F.softmax(logits, dim=1)[:,1]\n",
    "    for i, text in enumerate(batch):\n",
    "        print(f\"Text: {text}\\nScore: {scores[i].item()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be15a159d9874788f7b7854451912393d9e82d0d2bc47d83a870bda7fd9bc22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
