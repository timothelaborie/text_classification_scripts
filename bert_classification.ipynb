{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: UNSLOTH_COMPILE_DISABLE=1\n"
     ]
    }
   ],
   "source": [
    "# needed to fix a bug with unsloth\n",
    "%env UNSLOTH_COMPILE_DISABLE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.5: Fast Modernbert patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.999 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:395834371\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, FastModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import TrainingArguments, Trainer, ModernBertModel, AutoModelForSequenceClassification, training_args\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = 'answerdotai/ModernBERT-large'\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = model_name,load_in_4bit = False,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    auto_model = AutoModelForSequenceClassification,\n",
    "    num_labels = NUM_CLASSES,\n",
    ")\n",
    "print(\"model parameters:\" + str(sum(p.numel() for p in model.parameters())))\n",
    "\n",
    "# make all parameters trainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found [here](https://github.com/timothelaborie/text_classification_scripts/blob/main/data/finance_sentiment_multiclass.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dd1859a86d47f7ae5ac7e54dec68c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed85ecfce1f4054ab7fc4f6135027db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/433 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3893\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_DIR + \"finance_sentiment_multiclass.csv\")\n",
    "\n",
    "labels = data[\"label\"].tolist()\n",
    "labels = [x-1 for x in labels]\n",
    "# convert labels to one hot vectors\n",
    "labels = np.eye(3)[labels]\n",
    "\n",
    "train_data,val_data, train_labels, val_labels = train_test_split(data[\"text\"], labels, test_size=0.1, random_state=42)\n",
    "dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(train_data, train_labels)])\n",
    "val_dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(val_data, val_labels)])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothe\\AppData\\Local\\Temp\\ipykernel_66148\\29605135.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,893 | Num Epochs = 3 | Total steps = 366\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 395,834,371/395,834,371 (100.00% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='366' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [366/366 01:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.564900</td>\n",
       "      <td>0.430882</td>\n",
       "      <td>0.713626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.308327</td>\n",
       "      <td>0.812933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.288410</td>\n",
       "      <td>0.822171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        optim=training_args.OptimizerNames.ADAMW_TORCH,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        num_train_epochs=3, # bert-style models usually need more than 1 epoch\n",
    "        save_strategy=\"epoch\",\n",
    "\n",
    "        # report_to=\"wandb\",\n",
    "        report_to=\"none\",\n",
    "\n",
    "        group_by_length=True,\n",
    "\n",
    "        # eval_strategy=\"no\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.25,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=0.25,\n",
    "        \n",
    "    ),\n",
    "    compute_metrics=lambda eval_pred: { \"accuracy\": accuracy_score(eval_pred[1].argmax(axis=-1), eval_pred[0].argmax(axis=-1)) }\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "model = model.eval()\n",
    "FastLanguageModel.for_inference(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:01<00:00, 11.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 82.45% (357/433)\n",
      "\n",
      "--- Random samples ---\n",
      "\n",
      "Text: Turkey Stiffens Manipulation Penalties in Banking Overhaul\n",
      "True: 0  Pred: 0 âœ…\n",
      "Probs: 0: 0.846, 1: 0.114, 2: 0.040\n",
      "\n",
      "Text: The Manitowoc Company, Inc. Just Reported Earnings, And Analysts Cut Their Target Price\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.065, 1: 0.029, 2: 0.906\n",
      "\n",
      "Text: $BLMN $EAT $SBUX - Restaurants stocks break higher, analysts reel in near-term expectations https://t.co/fOjVVJdfF0\n",
      "True: 1  Pred: 1 âœ…\n",
      "Probs: 0: 0.004, 1: 0.974, 2: 0.021\n",
      "\n",
      "Text: $CMCSA $LHX - Comcast sues L3Harris in patent dispute https://t.co/kWReshGbvz\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.023, 1: 0.035, 2: 0.942\n",
      "\n",
      "Text: Libyan economic experts will study the distribution of crucial oil revenue as efforts continue to solve the war-ravâ€¦ https://t.co/S9lmpnDTqJ\n",
      "True: 0  Pred: 0 âœ…\n",
      "Probs: 0: 0.888, 1: 0.013, 2: 0.099\n",
      "\n",
      "Text: Stocks Suffer 'Shocking' Down Week As Fed Balance Sheet Unexpectedly Shrinks https://t.co/bspsRi3Wow\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.005, 1: 0.001, 2: 0.993\n",
      "\n",
      "Text: Burger King says it never promised Impossible Whoppers were vegan https://t.co/oZCnoupsYV https://t.co/lauoccNH0n\n",
      "True: 0  Pred: 0 âœ…\n",
      "Probs: 0: 0.762, 1: 0.070, 2: 0.167\n",
      "\n",
      "Text: McEwen Mining prices public offering at $1.325/unit\n",
      "True: 0  Pred: 0 âœ…\n",
      "Probs: 0: 0.710, 1: 0.284, 2: 0.006\n",
      "\n",
      "Text: H&P downgraded at Argus as drilling industry weakness seen persisting\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.000, 1: 0.000, 2: 1.000\n",
      "\n",
      "Text: $SPLK - Splunk: Full Steam Ahead. Follow this and any other stock on Seeking Alpha! https://t.co/DCvnAuSOBa #markets #stocks #economy\n",
      "True: 0  Pred: 1 âŒ\n",
      "Probs: 0: 0.040, 1: 0.959, 2: 0.001\n",
      "\n",
      "Text: Three dead in shooting at Oklahoma Walmart: RPT\n",
      "True: 0  Pred: 2 âŒ\n",
      "Probs: 0: 0.036, 1: 0.001, 2: 0.964\n",
      "\n",
      "Text: U.S. Oil Inventories Rise More Than Expected #WTI #Stock #MarketScreener https://t.co/lMkNlbjinO https://t.co/wBBq3HdLZO\n",
      "True: 2  Pred: 1 âŒ\n",
      "Probs: 0: 0.002, 1: 0.997, 2: 0.001\n",
      "\n",
      "Text: Casper Sleep stock languishes below IPO issue price after falling 5%\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.000, 1: 0.000, 2: 1.000\n",
      "\n",
      "Text: The global oil market is drowning in excess crude as demand plummets. Insights via @CMEGroup https://t.co/JklSJKFfRS\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.001, 1: 0.001, 2: 0.998\n",
      "\n",
      "Text: Asia Stocks Open Mixed as Trade Details Awaited: Markets Wrap\n",
      "True: 0  Pred: 1 âŒ\n",
      "Probs: 0: 0.023, 1: 0.865, 2: 0.112\n",
      "\n",
      "Text: From Starbucks to Seattle, companies and cities alike are banning plastic straws. Are takeout containers next?â€¦ https://t.co/Ew4Fsl6K0m\n",
      "True: 0  Pred: 0 âœ…\n",
      "Probs: 0: 0.968, 1: 0.003, 2: 0.029\n",
      "\n",
      "Text: Americans' outlook on the economy faltered significantly last month as the coronavirus crisis began to take hold inâ€¦ https://t.co/5jeCXLXrrR\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.002, 1: 0.002, 2: 0.996\n",
      "\n",
      "Text: Boris Johnsonâ€™s Conservative Party will pledge not to increase several key tax measures if it wins next monthâ€™s genâ€¦ https://t.co/RENnMT4Dtr\n",
      "True: 0  Pred: 0 âœ…\n",
      "Probs: 0: 0.999, 1: 0.001, 2: 0.001\n",
      "\n",
      "Text: Casper Sleep shares slide 5% to trade at $10.46, below $12 IPO price\n",
      "True: 2  Pred: 2 âœ…\n",
      "Probs: 0: 0.000, 1: 0.000, 2: 1.000\n",
      "\n",
      "Text: Brixmor 2020 FFO guidance comes in on the light side\n",
      "True: 2  Pred: 0 âŒ\n",
      "Probs: 0: 0.868, 1: 0.131, 2: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "correct = 0\n",
    "results = []\n",
    "\n",
    "# If your val_labels are one-hot, convert to class indices\n",
    "if isinstance(val_labels, np.ndarray) and val_labels.ndim == 2:\n",
    "    val_true_labels = np.argmax(val_labels, axis=1)\n",
    "else:\n",
    "    val_true_labels = val_labels\n",
    "\n",
    "val_texts = list(val_data)\n",
    "val_true_labels = list(val_true_labels)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(val_texts), batch_size), desc=\"Evaluating\"):\n",
    "        batch_texts = val_texts[i:i+batch_size]\n",
    "        batch_labels = val_true_labels[i:i+batch_size]\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1).cpu().numpy()\n",
    "        # Count correct\n",
    "        correct += np.sum(preds == batch_labels)\n",
    "        # Store results for display\n",
    "        for j in range(len(batch_texts)):\n",
    "            results.append({\n",
    "                \"text\": batch_texts[j][:200],\n",
    "                \"true\": batch_labels[j],\n",
    "                \"pred\": preds[j],\n",
    "                \"probs\": probs[j].detach().float().cpu().numpy(),\n",
    "                \"ok\": preds[j] == batch_labels[j]\n",
    "        })\n",
    "\n",
    "accuracy = 100 * correct / len(val_texts)\n",
    "print(f\"\\nValidation accuracy: {accuracy:.2f}% ({correct}/{len(val_texts)})\")\n",
    "\n",
    "# Show a few random samples\n",
    "import random\n",
    "display = 20\n",
    "print(\"\\n--- Random samples ---\")\n",
    "for s in random.sample(results, min(display, len(results))):\n",
    "    print(f\"\\nText: {s['text']}\")\n",
    "    print(f\"True: {s['true']}  Pred: {s['pred']} {'âœ…' if s['ok'] else 'âŒ'}\")\n",
    "    print(\"Probs:\", \", \".join([f\"{k}: {v:.3f}\" for k, v in enumerate(s['probs'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# stop running all cells\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# stop running all cells\n",
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to load the model again (run every cell above the one where the trainer is called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last checkpoint: trainer_output\\checkpoint-244\n",
      "==((====))==  Unsloth 2025.4.5: Fast Modernbert patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.999 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "output_dir = \"trainer_output\"\n",
    "last_checkpoint = get_last_checkpoint(output_dir)\n",
    "print(\"Last checkpoint:\", last_checkpoint)\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = last_checkpoint,load_in_4bit = False,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    auto_model = AutoModelForSequenceClassification,\n",
    "    num_labels = NUM_CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0579, -0.5859, -1.1719]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "print(model(input_ids=tensor([[1,2,3,4,5]]).cuda(), attention_mask=tensor([[1,1,1,1,1]]).cuda()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be15a159d9874788f7b7854451912393d9e82d0d2bc47d83a870bda7fd9bc22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
