{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "# Text classification with Unsloth\n",
        "\n",
        "This modified Unsloth notebook trains an LLM on any text classification dataset, where the input is a csv with columns \"text\" and \"label\".\n",
        "\n",
        "### Added features:\n",
        "\n",
        "- Trims the classification head to contain only the number tokens such as \"1\", \"2\" etc, which saves 1 GB of VRAM, allows you to train the head without massive memory usage, and makes the start of the training session more stable.\n",
        "- Only the last token in the sequence contributes to the loss, the model doesn't waste its capacity by trying to predict the input\n",
        "- includes \"group_by_length = True\" which speeds up training significantly for unbalanced sequence lengths\n",
        "- Efficiently evaluates the accuracy on the validation set using batched inference\n",
        "\n",
        "### Update 4th of May 2025:\n",
        "\n",
        "- Added support for more than 2 classes\n",
        "- The classification head is now built back up to the original size after training, no more errors in external libraries.\n",
        "- Made the batched inference part much faster and cleaner\n",
        "- Changed model to Qwen 3\n",
        "- Improved comments to explain the complicated parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# needed as this function doesn't like it when the lm_head has its size changed\n",
        "from unsloth import tokenizer_utils\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Major: 8, Minor: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
            "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.4.5: Fast Qwen3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.999 GB. Platform: Windows.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5f86bbe4b2841c586205f7324dfa2e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from typing import Tuple\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "NUM_CLASSES = 3 # number of classes in the csv\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "# model_name = \"unsloth/Qwen3-4B-Base\";load_in_4bit = False\n",
        "model_name = \"Qwen3-4B-Base\";load_in_4bit = False\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,load_in_4bit = load_in_4bit,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now trim the classification head so the model can only say numbers 0-NUM_CLASSES and no other words. (We don't use 0 here but keeping it makes everything simpler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2560])\n",
            "torch.Size([151936, 2560])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{15: 0, 16: 1, 17: 2, 18: 3}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "# keep only the number tokens from lm_head\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
        "reverse_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.4.5 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Training lm_head in mixed precision to save VRAM\n",
            "trainable parameters: 33040384\n"
          ]
        }
      ],
      "source": [
        "from peft import LoftQConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"lm_head\", # can easily be trained because it now has a small size\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    # init_lora_weights = 'loftq',\n",
        "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
        ")\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4000\n"
          ]
        }
      ],
      "source": [
        "kaggle = os.getcwd() == \"/kaggle/working\"\n",
        "input_dir = \"/kaggle/input/whatever/\" if kaggle else \"data/\"\n",
        "data = pd.read_csv(input_dir + \"finance_sentiment_multiclass.csv\") # columns are text,label\n",
        "\n",
        "train_size = 4000\n",
        "val_size = 320\n",
        "\n",
        "data_sample = data.sample(n=train_size+val_size, random_state=42)\n",
        "train_df, val_df = train_test_split(data_sample, test_size=val_size/len(data_sample), random_state=42)\n",
        "train_dataset = datasets.Dataset.from_pandas(train_df,preserve_index=False)\n",
        "print(len(train_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "token_counts = [len(tokenizer.encode(x)) for x in train_df.text]\n",
        "# plot the token counts\n",
        "a = plt.hist(token_counts, bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "e0ea75df3b5c49ce89427e4d245a7646",
            "02237d111eba4155ab5a48a1b33d82a4",
            "927e1d33248d4653a785049b50b6d814",
            "09979ecafec9443ba1f7335ed64de778",
            "088a000de2234c9f94a8b09e8a8abbb2",
            "38c7b4ead56b4540bf68fbe3a5496b9c",
            "9a0787c7c98b49b8a9141e5212faa249",
            "daf62811f5384d0f9aea58b40f23161a",
            "6f90172dd5c240c885140d49add6208f",
            "187078f1978843f2b873cee2ee55ac91",
            "ca8af702ee764e7fa620476854a4f2cf",
            "80b9d976e30e40ecaf35a606bca5d647",
            "e9b423d370314fdfa3c22e95c3a35d92",
            "307f44c0a4da4d3ca0d44b54f9a5f6c0",
            "5503539430024c7586d4f8589c92fd74",
            "b17542f2cd0f45e79e755df622328358",
            "fe6d1bab4fd042b5af89f6bb8a73cc39",
            "8cb7748eaa354d4ea0e3222685f1d9b4",
            "53bfefa3e2c04cce9ab7d2b3fbca59d9",
            "aa58f2124335451890857ded72414ecf",
            "b331b446e257441387b331d822f570a0",
            "49e3298fbe054c7abb6a041ecd63737c",
            "6b104e7242cf4a608cfd0c4fc6f44db1",
            "1a6288e052884d5aa486415880acf134",
            "289c8b4d1686443f8bed7441673a2c15",
            "1318a594d2df47779b84f22e4b0c0702",
            "355ecaa94d1a43b09e1d0b3f6e6ac8e3",
            "156432d2f1b74697bd74604e1bd5bf13",
            "976fa37d0fa14282bc96b3c948b9da94",
            "24768bbd587d4b1ba09b6c52dcd6237c",
            "cdf054995f314976b7124de9564cbd9b",
            "1eec25504a6f41d199940c4232b0a114",
            "007a9dbfb5c84efd8ffd801bc04c0c20",
            "2ffddabe91124b279f9a3ac41d1ead9d",
            "f97edc77840e49c49aed8e7ab80f45d1",
            "33761916bce24fde9bcff866736ddad7",
            "726aae420987454e87a08b9314844895",
            "071858b3801e4844a4fc5a91205335c4",
            "6a98441b0ff74361a79c96178834d8bc",
            "40e8b59e229548c68ccb339234a5e525",
            "56b011c2482b427483508e259dc231fd",
            "507c15541fe7489aa1fd1fd81c4aa222",
            "4bf05ee528bb4bb2979cde4d0a09544b",
            "d62a5696b9704580b69e6fc3be9ffa8a"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "26e1bc8e-c4e8-472e-ca91-670e3381c6b3"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Here is a financial news:\n",
        "{}\n",
        "\n",
        "Classify this news into one of the following:\n",
        "class 1: Bullish\n",
        "class 2: Neutral\n",
        "class 3: Bearish\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class {}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(dataset_):\n",
        "    # this is to fix an issue with the transformers library where the first time this function is called, it is called with a string for some reason\n",
        "    if isinstance(dataset_['text'], str):\n",
        "        return [\" \"]*100\n",
        "        \n",
        "    texts = []\n",
        "    for i in range(len(dataset_['text'])):\n",
        "        text_ = dataset_['text'][i]\n",
        "        label_ = dataset_['label'][i] # the csv is setup so that the label column corresponds exactly to the 3 classes defined above in the prompt (important)\n",
        "\n",
        "        text = prompt.format(text_, label_)\n",
        "\n",
        "        texts.append(text)\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this custom collator makes it so the model trains only on the last token of the sequence. It also maps from the old tokenizer to the new lm_head indices\n",
        "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        mlm: bool = False,\n",
        "        ignore_index: int = -100,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, mlm=mlm, **kwargs)\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        for i in range(len(examples)):\n",
        "            # Find the last non-padding token\n",
        "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
        "            # Set all labels to ignore_index except for the last token\n",
        "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
        "            # If the last token in the text is, for example, \"2\", then this was processed with the old tokenizer into number_token_ids[2]\n",
        "            # But we don't actually want this because number_token_ids[2] could be something like 27, which is now undefined in the new lm_head. So we map it to the new lm_head index.\n",
        "            batch[\"labels\"][i, last_token_idx] = reverse_map[ batch[\"labels\"][i, last_token_idx].item() ]\n",
        "\n",
        "\n",
        "        return batch\n",
        "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "477d5041a08f4a3a9f7cfa1d98ab48ff",
            "25eecdf89b8845a989ce2c8c4d9edbeb",
            "e02470ee64ad4f0fb2160b088cdcba7f",
            "b7c0858a80684aed9c19ce00dda85815",
            "d0b82646d6f549d7ad59e9ff5f426e88",
            "d94a8d1f9d294abbb156e39da065910f",
            "ebb0f92e750447a39ff23a23ea73b445",
            "d4d01dc3290b4174ab13454a28faf972",
            "c6941f8c60ef49a8a79ed265c46652c2",
            "24aa300026334db1b545bf0fa906112e",
            "6d0d021108b54147a62e50fea1ba88ea",
            "deb2982b19764ed5aec0b4d80e776279",
            "c24904c0a7294f3a93bb64aa38e70316",
            "26a77ab74a4a4e21b9afd9798a9f9a29",
            "7189bac8d0474bcea50cf8711259516c",
            "ce81350896a44331aa6b6960b7370325",
            "73d4af57ddc64fb3afd0e2ab068cbcb4",
            "672d990adee44df6b58e27fe5804986f",
            "3fe95fc9bc034a2db85ed19aedfd4250",
            "c1c0053b8e674ed6ac81316d4af82c48",
            "74a0e53405cd4d64bd597ad5461ed5dd",
            "278d35f6e08d45e2a49c3509dd442f0c",
            "b19d83c6f5ad4f04941e6a684678ac07",
            "88e100a175e64a3dab6f772847deffd6",
            "457b5df3a4294c8a966e233d34c15a82",
            "18af2c44a24b4aaabfd192e3fc4bf655",
            "61944d9473394be5b19cfd48fd504481",
            "68d18369acc540a594aef61a2de07e63",
            "69c676782e0b496b820b55c45424177d",
            "08dcaabc623c4759a00fbee5430e3ba9",
            "45a3bcaffc3f4184b9ae869f7b0ccef4",
            "c45f02fbb40e4041ba7f95074f8e1e82",
            "0fc1037a17e541eba92a2d6f400ac6eb",
            "7bd2cc9aa724408fa9e70795744cad85",
            "fb0588bffd7a4238bac3f73052e09335",
            "68ff2006b3794e23b4ccbc2c83c52b9b",
            "910f2e6fffd24cd7b6e68c932c2a2524",
            "6345dc6f40c6444aa05ed2aba7809a3c",
            "92eab7fadbed4bc2bc2935b4e3d800cf",
            "2dfef2cd79c8463cb7eadad6a04aba91",
            "3be5c37493e742aebf0aa29b6723283b",
            "273be47263384901b6cf9f249ee3409d",
            "2ff515b72bbb43c889e2172c83933803",
            "a1cd830a712d490181d176267ce7b6f0",
            "1a8da471604841ec9f6c8e0073471c00",
            "f899ae16708544379d63960be07b7c32",
            "0bdbf9b92f7b4925a5ac28df93fd0fb0",
            "c1d8820a789f4899a839e6d28a4f333c",
            "e711d7f85eee4fe195fad9fbddcfece2",
            "56ed5fd876d94ebbaa8b4e905c348a0d",
            "5d461f10bdc44c6b95d070fa9d7425d1",
            "f4e0e7a39ad9484f930e6673c35c2f5d",
            "baad9118cade4cee9600a7fbf6426e0a",
            "16fac1aad22444c4ad42ad0e6e1dfdc9",
            "ac35368de2d746b4bed736459d187dbd"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "adb8cb5d-0ec3-4b79-83a7-5691873873e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "734c3da7486a4428abe8f82a64c7e9ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False, # not needed because group_by_length is True\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        # report_to = \"wandb\",\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "    ),\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    data_collator=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "815b67fb-14a2-43ab-d587-3cbe038b4349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA GeForce RTX 3090. Max memory = 23.999 GB.\n",
            "8.41 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "16039f41-2abb-44e6-f020-4e1d4fcc0102"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 4,000 | Num Epochs = 1 | Total steps = 125\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 33,040,384/4,055,518,720 (0.81% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 05:30, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.684900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.167400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.191000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.096000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.093100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.924200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.847800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.926600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.848900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.840200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.862900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.674700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.576100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.552400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.474800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.517800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.624700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.397900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.429500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.409400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.447700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.301800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.323300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.270500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.451600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.486100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.424200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.413100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.498300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.598100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.509700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.523400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.367300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.793500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.354000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.238300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.263800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.747800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.209100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.211200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.628400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.365300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.598400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.232200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.320400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.395200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.331400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.475100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.269100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.237700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.205500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.432200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.251600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.428400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.675200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.391900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.630200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.544300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.347200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.347600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.387800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.318300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.270100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.401100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.715300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.307900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.323300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.215600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.215200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.238800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.437000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.350700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.241000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.478800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.239300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.454600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.324500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.254300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.238200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.367500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.527500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.510200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.399200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.633000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.373300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.193900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.400200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.339200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.370900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.248400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.477400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.163900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.304200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.288800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.234900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.537900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.153700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.309400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.158200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.197000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.194900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.375800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.299200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.478400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.620100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.392800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.410100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "ff1b0842-5966-4dc2-bd98-c20832526b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "341.2138 seconds used for training.\n",
            "5.69 minutes used for training.\n",
            "Peak reserved memory = 9.082 GB.\n",
            "Peak reserved memory for training = 0.672 GB.\n",
            "Peak reserved memory % of max memory = 37.843 %.\n",
            "Peak reserved memory for training % of max memory = 2.8 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "This part evaluates the model on the val set with batched inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### remake the old lm_head but with unused tokens having -100 weights (improves compatibility with libraries like vllm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remade lm_head: shape = torch.Size([151936, 2560]). Allowed tokens: [15, 16, 17, 18]\n"
          ]
        }
      ],
      "source": [
        "# Save the current (trimmed) lm_head\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), -100.0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "\n",
        "# Fill in the weights for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight\n",
        "with torch.no_grad():\n",
        "    # Create a new linear layer with the same input dimension but full vocab size\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=False, device=model.device)\n",
        "    # Set its weights\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    # Replace the module\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n",
        "\n",
        "saved_name = f\"lora_model_{model_name.replace('/','_')}\"\n",
        "model.save_pretrained(saved_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batched Inference on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation accuracy: 83.44% (267/320)\n",
            "\n",
            "--- Random samples ---\n",
            "\n",
            "Text: Former FedEx manager pleads guilty in package theft scheme\n",
            "True: 1  Pred: 3 ❌\n",
            "Probs: 1: 0.217, 2: 0.026, 3: 0.757\n",
            "\n",
            "Text: Futures, Global Markets Soar For Second Day As Virus Fears Fade, Dollars Slides https://t.co/ORNlxqQtf5\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.220, 2: 0.766, 3: 0.014\n",
            "\n",
            "Text: Target Hospitality downgraded to perform from outperform at Oppenheimer\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.020, 2: 0.008, 3: 0.971\n",
            "\n",
            "Text: Brazil's central bank stepped in to prop up the currency https://t.co/yQBoa6mTqi\n",
            "True: 1  Pred: 2 ❌\n",
            "Probs: 1: 0.429, 2: 0.486, 3: 0.085\n",
            "\n",
            "Text: Why ADMA Biologics Is Tumbling 13.5% Today\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.002, 2: 0.003, 3: 0.995\n",
            "\n",
            "Text: $AWK - American Water CEO Story to retire in April; COO Lynch to step up https://t.co/x7EmbvuvgB\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.835, 2: 0.037, 3: 0.128\n",
            "\n",
            "Text: Earnings Update: electroCore, Inc. Just Reported And Analysts Are Trimming Their Forecasts\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.145, 2: 0.022, 3: 0.833\n",
            "\n",
            "Text: Stocks Finish Lower After Paring Losses on U.S.-Mexico-Canada Trade Pact #SP500 #index #MarketScreener… https://t.co/RlFBEx234I\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.029, 2: 0.015, 3: 0.956\n",
            "\n",
            "Text: $LOW - Lowe's racks up another positive rating despite recession risk https://t.co/ekx4f6aqIK\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.026, 2: 0.967, 3: 0.007\n",
            "\n",
            "Text: Capacity Utilisation In Manufacturing Sector Drops To Lowest In A Decade, RBI Survey Shows\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.011, 2: 0.010, 3: 0.980\n",
            "\n",
            "Text: Government data showed hiring saw the biggest surge in 10 months in November. Employers added a much stronger-than-… https://t.co/vGvxANUBQE\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.094, 2: 0.892, 3: 0.014\n",
            "\n",
            "Text: $USO $XLE $OIL - Oil slides amid output uncertainty, forecast for big Cushing stockpile gain https://t.co/1EV0cQBSx6\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.047, 2: 0.015, 3: 0.938\n",
            "\n",
            "Text: $DIS - Disney -1.5% as Wells moves to side on parks worries https://t.co/KpVmXMYUHb\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.091, 2: 0.046, 3: 0.863\n",
            "\n",
            "Text: Euro Slumps to 4-Month Low Amid Weaker Data, Virus Fallout Fears\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.020, 2: 0.014, 3: 0.966\n",
            "\n",
            "Text: China's GDP growth this quarter will be 0%, according to top economist Ed Hyman\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.093, 2: 0.021, 3: 0.886\n",
            "\n",
            "Text: AMD stock hits highest level in 13 years after launch of promising graphics card\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.014, 2: 0.981, 3: 0.005\n",
            "\n",
            "Text: HSBC Says British Pound May Soar. Or Crash\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.775, 2: 0.153, 3: 0.072\n",
            "\n",
            "Text: $XLE: Sector Briefing: Energy https://t.co/TRUxGKAme7\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.948, 2: 0.032, 3: 0.020\n",
            "\n",
            "Text: Hedge Funds Are Crazy About PC Connection, Inc. (CNXN)\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.067, 2: 0.920, 3: 0.013\n",
            "\n",
            "Text: U.S. Jobs Top Estimates With 225,000 Gain, Wages Accelerate\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.370, 2: 0.610, 3: 0.020\n",
            "\n",
            "Text: $XPO - XPO Logistics: Why You Shouldn't Let Cyclicality Dictate Investment Decisions. Read more and sign up for upd… https://t.co/mnOOzwmdQy\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.879, 2: 0.082, 3: 0.039\n",
            "\n",
            "Text: Hammond Power Solutions declares CAD 0.07 dividend\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.891, 2: 0.044, 3: 0.065\n",
            "\n",
            "Text: California department to reject Sezzle's application for lending license in state\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.020, 2: 0.011, 3: 0.969\n",
            "\n",
            "Text: HR Confidential: I fired her. Then she revealed all of the office's sordid sex-and-drugs stories. (via @CNBCMakeIt) https://t.co/Pkyk6k5nkg\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.936, 2: 0.017, 3: 0.047\n",
            "\n",
            "Text: Dow industrials down 50 points as Tuesday morning’s gains evaporate\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.009, 2: 0.005, 3: 0.986\n",
            "\n",
            "Text: $PACD IS BREAKING OUT ON MASSIVE VOLUME. SHORT SQUEEZE INTO THE CLOSE.\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.263, 2: 0.715, 3: 0.022\n",
            "\n",
            "Text: 7-Eleven pilots first cashierless store\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.848, 2: 0.115, 3: 0.037\n",
            "\n",
            "Text: Dow industrials end around 175 points higher, up 0.6%\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.004, 2: 0.994, 3: 0.002\n",
            "\n",
            "Text: Highlight: “If you think about the value that LVMH might be able to unlock for Tiffany, it does justify the price t… https://t.co/7YlpHgIQ3H\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.835, 2: 0.128, 3: 0.037\n",
            "\n",
            "Text: KAR Auction Services gains on M&A speculation\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.037, 2: 0.954, 3: 0.009\n",
            "\n",
            "Text: Trump’s Acquittal Boxes In Republicans\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.622, 2: 0.084, 3: 0.294\n",
            "\n",
            "Text: Calculating The Fair Value Of SP Plus Corporation (NASDAQ:SP)\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.883, 2: 0.073, 3: 0.044\n",
            "\n",
            "Text: Central Bank of Philippines : Inflation Eases Further to 2.5 Percent in March #CentralBankofPhilippines #economy… https://t.co/BXqDnEXNqV\n",
            "True: 2  Pred: 1 ❌\n",
            "Probs: 1: 0.537, 2: 0.418, 3: 0.044\n",
            "\n",
            "Text: The RBA Turns Dovish, with Focus on Trade to Persist Throughout The Day\n",
            "True: 2  Pred: 1 ❌\n",
            "Probs: 1: 0.492, 2: 0.383, 3: 0.124\n",
            "\n",
            "Text: 'We know that there will be very likely some effects on the United States', said the Fed chairman Jay Powell about… https://t.co/eAeVR5GUm2\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.870, 2: 0.049, 3: 0.081\n",
            "\n",
            "Text: The underlying trend remains to the downside  #Stock #MarketScreener https://t.co/SnBOfWESuP https://t.co/aw02S6u7Io\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.042, 2: 0.011, 3: 0.948\n",
            "\n",
            "Text: Mesa Royalty Trust declares $0.036724557 dividend\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.891, 2: 0.044, 3: 0.065\n",
            "\n",
            "Text: The deepening crisis at debt-ridden Eskom shut down South Africa's key mining industry for 24 hours, hitting gold a… https://t.co/DXHzj6LIkd\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.211, 2: 0.053, 3: 0.736\n",
            "\n",
            "Text: EUR/USD Daily Forecast – Euro Rally Halted at Major Resistance\n",
            "True: 3  Pred: 1 ❌\n",
            "Probs: 1: 0.444, 2: 0.112, 3: 0.444\n",
            "\n",
            "Text: Citadel Dominates Multistrategy Peers With 3.4% Gain\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.041, 2: 0.934, 3: 0.025\n",
            "\n",
            "Text: Mexican president's nationalist oil vision fuels standoff with Saudis https://t.co/ps9r1I9rb1 https://t.co/ZSeWOI4XkO\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.705, 2: 0.035, 3: 0.259\n",
            "\n",
            "Text: Bank of Nova Scotia : Scotiabank CEO expects 'somewhat elevated' loan losses for 3-4 quart... #BankofNovaScotia… https://t.co/OAXfDtrtAk\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.074, 2: 0.024, 3: 0.902\n",
            "\n",
            "Text: $SGH: SMART Global beats by $0.02, beats on revs; guides Q3 EPS in-line, revs in-line https://t.co/dvUyxPP0qW\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.343, 2: 0.640, 3: 0.017\n",
            "\n",
            "Text: $JCP - One Way J.C. Penney Can Tackle Its Big Debt Problem. Read more and get updates on any stock!… https://t.co/3bj3sVOJuD\n",
            "True: 1  Pred: 1 ✅\n",
            "Probs: 1: 0.732, 2: 0.238, 3: 0.030\n",
            "\n",
            "Text: The biggest U.S. retail organization warns that imports are falling sharply -- a sign that supply chains will be di… https://t.co/CcPLQVseCe\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.176, 2: 0.035, 3: 0.789\n",
            "\n",
            "Text: Prospects for Aurora Cannabis Stock Look Grim\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.026, 2: 0.007, 3: 0.967\n",
            "\n",
            "Text: $CAT - Caterpillar sees more weak demand ahead as firms defer capital decisions https://t.co/CzAxE8TDL1\n",
            "True: 3  Pred: 3 ✅\n",
            "Probs: 1: 0.059, 2: 0.017, 3: 0.924\n",
            "\n",
            "Text: Highlight: “About 21 million investors in the U.S. would have an interest in investing in bitcoin,”… https://t.co/wJRda3ylEH\n",
            "True: 2  Pred: 1 ❌\n",
            "Probs: 1: 0.783, 2: 0.198, 3: 0.018\n",
            "\n",
            "Text: Prestige Brands Holdings EPS beats by $0.06, beats on revenue\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.243, 2: 0.749, 3: 0.008\n",
            "\n",
            "Text: $ALT strong all day on volume abv average. https://t.co/qW3dIm2dKa\n",
            "True: 2  Pred: 2 ✅\n",
            "Probs: 1: 0.029, 2: 0.962, 3: 0.009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Prepare inference prompt\n",
        "inference_prompt_template = prompt.split(\"class {}\")[0] + \"class \"\n",
        "\n",
        "# Sort validation set by length for efficient batching\n",
        "val_df['token_length'] = val_df['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
        "val_df_sorted = val_df.sort_values(by='token_length').reset_index(drop=True)\n",
        "\n",
        "display = 50\n",
        "batch_size = 16\n",
        "device = model.device\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for i in tqdm(range(0, len(val_df_sorted), batch_size), desc=\"Evaluating\"):\n",
        "        batch = val_df_sorted.iloc[i:i+batch_size]\n",
        "        prompts = [inference_prompt_template.format(text) for text in batch['text']]\n",
        "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(device)\n",
        "        logits = model(**inputs).logits\n",
        "        last_idxs = inputs.attention_mask.sum(1) - 1\n",
        "        last_logits = logits[torch.arange(len(batch)), last_idxs, :]\n",
        "        probs_all = F.softmax(last_logits, dim=-1)\n",
        "        probs = probs_all[:, number_token_ids] # only keep the logits for the number tokens\n",
        "        preds = torch.argmax(probs, dim=-1).cpu().numpy() # looks like [1 1 1 1 3 1 3 1 3 1 1 1 1 2 2 3]\n",
        "\n",
        "        true_labels = batch['label'].tolist()\n",
        "        correct += sum([p == t for p, t in zip(preds, true_labels)])\n",
        "        # Store a few samples for display\n",
        "        for j in range(len(batch)):\n",
        "            results.append({\n",
        "                \"text\": batch['text'].iloc[j][:200],\n",
        "                \"true\": true_labels[j],\n",
        "                \"pred\": preds[j],\n",
        "                \"probs\": probs[j][1:].float().cpu().numpy(), # ignore prob for class 0 and convert from tensor to float\n",
        "                \"ok\": preds[j] == true_labels[j]\n",
        "            })\n",
        "\n",
        "accuracy = 100 * correct / len(val_df_sorted)\n",
        "print(f\"\\nValidation accuracy: {accuracy:.2f}% ({correct}/{len(val_df_sorted)})\")\n",
        "\n",
        "print(\"\\n--- Random samples ---\")\n",
        "for s in random.sample(results, min(display, len(results))):\n",
        "    print(f\"\\nText: {s['text']}\")\n",
        "    print(f\"True: {s['true']}  Pred: {s['pred']} {'✅' if s['ok'] else '❌'}\")\n",
        "    print(\"Probs:\", \", \".join([f\"{k}: {v:.3f}\" for k, v in enumerate(s['probs'], start=1)]))\n",
        "\n",
        "# Clean up\n",
        "if 'token_length' in val_df:\n",
        "    del val_df['token_length']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# stop running all cells\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "# stop running all cells\n",
        "1/0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if you closed the notebook kernel and want to reload the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
            "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.4.5: Fast Qwen3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.999 GB. Platform: Windows.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e61da3dc971e4d929cc4b39a5de125f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.4.5 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "[\"Here is a financial news:\\nFor the global oil market, the coronavirus epidemic couldn't have hit a worse place\\n\\nClassify this news into one of the following:\\nclass 1: Bullish\\nclass 2: Neutral\\nclass 3: Bearish\\n\\nSOLUTION\\nThe correct answer is: class 3\"]\n"
          ]
        }
      ],
      "source": [
        "# load the model\n",
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"lora_model_Qwen3-4B-Base\",\n",
        "    load_in_4bit = False,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        ")\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "prompt = \"\"\"Here is a financial news:\n",
        "For the global oil market, the coronavirus epidemic couldn't have hit a worse place\n",
        "\n",
        "Classify this news into one of the following:\n",
        "class 1: Bullish\n",
        "class 2: Neutral\n",
        "class 3: Bearish\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class \"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True)\n",
        "decoded = tokenizer.batch_decode(outputs)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Llama 7b [free Kaggle](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5081962,
          "sourceId": 8512897,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
